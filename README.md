
# ğŸŒ€ Projeto Airflow + Docker + PostgreSQL

Este projeto tem como objetivo demonstrar uma arquitetura simples e funcional para orquestraÃ§Ã£o de pipelines utilizando **Apache Airflow** em conjunto com **Docker** e **PostgreSQL**. A stack Ã© ideal para fluxos de dados programÃ¡veis, com controle total sobre agendamentos, logs e estados de execuÃ§Ã£o.

**Banco de dados utlizado do Projeto [AED_DBT](https://github.com/Prog-LucasAlves/AED_DBT)**

---

## ğŸš€ Tecnologias Utilizadas

O projeto foi desenvolvido utilizando uma combinaÃ§Ã£o poderosa de tecnologias modernas, garantindo robustez, escalabilidade e facilidade de manutenÃ§Ã£o para orquestraÃ§Ã£o de pipelines de dados. Abaixo estÃ£o as principais tecnologias empregadas:

- ğŸ”§ **Apache Airflow**
Apache Airflow Ã© uma plataforma de cÃ³digo aberto usada para programar, monitorar e gerenciar workflows de dados. Neste projeto, o Airflow atua como o orquestrador principal, responsÃ¡vel por executar DAGs (Directed Acyclic Graphs) que representam fluxos de tarefas. Com ele, Ã© possÃ­vel definir dependÃªncias, agendamentos, monitoramento e logging de cada etapa do processo.

- ğŸ³ **Docker**
Docker Ã© a base da arquitetura do projeto, permitindo empacotar todos os serviÃ§os em containers independentes e reproduzÃ­veis. Usamos docker-compose para orquestrar mÃºltiplos containers, facilitando a configuraÃ§Ã£o, execuÃ§Ã£o e escalabilidade do ambiente de desenvolvimento e produÃ§Ã£o. Isso garante que o projeto funcione de forma idÃªntica em qualquer mÃ¡quina.

- ğŸ˜ **PostgreSQL**
O PostgreSQL Ã© um banco de dados relacional robusto, seguro e open source. Neste projeto, ele funciona como banco de metadados do Airflow, armazenando informaÃ§Ãµes sobre DAGs, tarefas, execuÃ§Ãµes e estados. Ele tambÃ©m pode ser utilizado como fonte de dados em DAGs personalizadas, com consultas SQL sendo executadas diretamente a partir do Airflow.

- ğŸ“¦ **Python**
A linguagem principal utilizada no desenvolvimento das DAGs e na customizaÃ§Ã£o do Airflow Ã© o Python. Por meio de operadores como BashOperator, PythonOperator e PostgresOperator, Ã© possÃ­vel criar fluxos de trabalho altamente flexÃ­veis e integrados com outras tecnologias.

- ğŸ“ **Dockerfile e Docker Compose**
O Dockerfile define como o container do Airflow serÃ¡ construÃ­do, permitindo a instalaÃ§Ã£o de dependÃªncias personalizadas. JÃ¡ o docker-compose.yml organiza os serviÃ§os em rede (como Airflow Webserver, Scheduler e PostgreSQL), define volumes e garante que tudo seja inicializado na ordem correta, com comandos especÃ­ficos para inicializaÃ§Ã£o e persistÃªncia dos dados.

- ğŸ“„ **.env**
Utilizamos variÃ¡veis de ambiente atravÃ©s do arquivo .env para garantir flexibilidade e seguranÃ§a na configuraÃ§Ã£o do projeto. Isso permite separar dados sensÃ­veis do cÃ³digo e facilitar mudanÃ§as sem alterar diretamente os arquivos principais.

---

## ğŸ“ Estrutura do Projeto

```bash
airflow_project/
â”œâ”€â”€ dags/                    # Arquivos com os fluxos de trabalho (DAGs)
â”‚   â””â”€â”€ dag_etl.py
â”œâ”€â”€ sql                      # Arquivos Sql que serÃ£o executados pela (DAG)
â”œâ”€â”€ .env                     # VariÃ¡veis de ambiente
â”œâ”€â”€ .flake8                  # ConfiguraÃ§Ã£o flake8
â”œâ”€â”€ .gitignore               # Arquivos a serem ignorados
â”œâ”€â”€ pre-commit-config.yaml   # ConfiguraÃ§Ã£o precommit
â”œâ”€â”€ .python-version          # VersÃ£o do Python utilizada no projeto
â”œâ”€â”€ Dockerfile               # Imagem customizada do Airflow
â”œâ”€â”€ docker-compose.yml       # OrquestraÃ§Ã£o dos serviÃ§os
â”œâ”€â”€ pyproject.toml           # Lista de dependÃªncias do projeto
â”œâ”€â”€ README.md                # DocumentaÃ§Ã£o do projeto

```

---

## âš™ï¸ PrÃ©-requisitos

Antes de iniciar, Ã© necessÃ¡rio ter instalado:

- [Docker](https://www.docker.com/)
- [Docker Compose](https://docs.docker.com/compose/)

---

## ğŸ“¦ InstalaÃ§Ã£o e ExecuÃ§Ã£o
